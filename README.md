# üß† NeuralDoc: Local RAG System

> **Asistente inteligente de documentaci√≥n t√©cnica que respeta tu privacidad.**
> *Ejecutado 100% localmente con LLMs Open Source (TinyLlama/Phi-3).*

![Python](https://img.shields.io/badge/Python-3.11-3776AB?logo=python)
![FastAPI](https://img.shields.io/badge/FastAPI-0.109-009688?logo=fastapi)
![Streamlit](https://img.shields.io/badge/Frontend-Streamlit-FF4B4B?logo=streamlit)
![LangChain](https://img.shields.io/badge/Orchestration-LangChain-1C3C3C?logo=langchain)
![Ollama](https://img.shields.io/badge/AI-Ollama-000000?logo=ollama)

## üìñ Descripci√≥n

**NeuralDoc** es un sistema de **Retrieval-Augmented Generation (RAG)** dise√±ado para ingenieros, estudiantes y profesionales que necesitan consultar manuales t√©cnicos, papers de investigaci√≥n o documentaci√≥n legal extensa sin depender de la nube.

A diferencia de soluciones comerciales como GPT-4, NeuralDoc corre **on-premise** (en tu propia m√°quina), garantizando que tus datos privados nunca salgan de tu ordenador y sin costes por token.

Caracter√≠sticas de Ingenier√≠a
Ingenier√≠a de Prompts (Zero-shot): Optimizaci√≥n de instrucciones espec√≠ficas para "Small Language Models" (SLMs) para reducir alucinaciones y mejorar la precisi√≥n sin necesitar modelos pesados.

B√∫squeda Vectorial Eficiente: Implementaci√≥n de ChromaDB persistente con particionamiento de textos (Chunking) optimizado para documentos t√©cnicos.

Optimizaci√≥n de Recursos (CPU-First): Dise√±ado para correr en laptops est√°ndar. Tiempo de respuesta optimizado (<20s) utilizando modelos cuantizados (TinyLlama/Phi-3).

Clean Architecture: Separaci√≥n modular de responsabilidades:

routers/: Endpoints de la API.

services/: L√≥gica de negocio (RAG, ETL).

core/: Configuraciones y variables de entorno.

üöÄ Instalaci√≥n y Despliegue
Prerrequisitos
Python 3.10+ instalado.

Ollama instalado y corriendo en segundo plano.

1. Clonar el repositorio
Bash
git clone [https://github.com/tu-usuario/neural-doc.git](https://github.com/tu-usuario/neural-doc.git)
cd neural-doc

2. Configurar el entorno virtual
Es recomendable usar un entorno aislado para las dependencias.

Bash
# Windows
python -m venv venv
.\venv\Scripts\Activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate

3. Instalar dependencias
Bash
pip install -r requirements.txt

4. Configurar Modelos de IA (Ollama)
Descarga los modelos optimizados para ejecuci√≥n local:
Bash
# Modelo de lenguaje (Cerebro) - Opci√≥n r√°pida
ollama pull tinyllama
# Modelo de Embeddings (Motor de b√∫squeda vectorial)
ollama pull nomic-embed-text


5. Variables de Entorno
Crea un archivo .env en la ra√≠z del proyecto (basado en el ejemplo):

Ini, TOML

PROJECT_NAME="Neural Doc Search"
OLLAMA_BASE_URL="http://localhost:11434"
OLLAMA_MODEL="tinyllama"
CHROMA_DB_DIR="chroma_db"
üõ†Ô∏è Ejecuci√≥n
El sistema requiere dos terminales ejecut√°ndose simult√°neamente:



Terminal 1: Backend (API)
Bash
uvicorn app.main:app --reload
# La API estar√° disponible en [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)


Terminal 2: Frontend (UI)
Bash
streamlit run frontend.py
# La interfaz web se abrir√° autom√°ticamente en http://localhost:8501


üìÇ Estructura del Proyecto

üì¶ NeuralDoc
 ‚î£ üìÇ app
 ‚îÉ ‚î£ üìÇ core         # Configuraci√≥n (Settings, .env)
 ‚îÉ ‚î£ üìÇ routers      # Endpoints (Chat, Upload)
 ‚îÉ ‚î£ üìÇ services     # L√≥gica RAG y Vector DB
 ‚îÉ ‚îó üìú main.py      # Entry point FastAPI
 ‚î£ üìú frontend.py    # Interfaz de usuario (Streamlit)
 ‚î£ üìú requirements.txt
 ‚î£ üìú docker-compose.yml
 ‚îó üìú README.md


ü§ù Contribuciones
Este proyecto fue desarrollado como parte de un portafolio de Ingenier√≠a Civil en Computaci√≥n. Las sugerencias y Pull Requests son bienvenidos.

Hecho con üêç y ‚òï por Gustavo Sanchez